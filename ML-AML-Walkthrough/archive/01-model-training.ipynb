{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Lab 1 - Training a Machine Learning Model"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "In this lab you will setup the Azure Machine Learning service and use it for tracking training of a `scikit-learn` model.\n\n![AML Arch](https://github.com/jakazmie/images-for-hands-on-labs/raw/master/amlarch.png)\n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Get the lab datasets\nThe following cell will download the dataset used by this lab. Click into the following cell and use `Shift + Enter` to execute it"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import os\n\n# Create a temporary folder to store locally relevant content for this notebook\ndatasetsFolderName = '../datasets'\nos.makedirs(datasetsFolderName, exist_ok=True)\nprint('Content files will be saved to {0}'.format(datasetsFolderName))\n\nfilesToDownload = ['UsedCars_Clean.csv', 'UsedCars_Affordability.csv']\n\nfor fileToDownload in filesToDownload:\n  downloadCommand = 'wget -O ''{0}/{1}'' ''https://databricksdemostore.blob.core.windows.net/data/aml-labs/{1}'''.format(datasetsFolderName, fileToDownload)\n  print(downloadCommand)\n  os.system(downloadCommand)\n  \n#List all downloaded files\nos.listdir(datasetsFolderName)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Train a simple model"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The following cell loads the sampled dataset. Use `Shift + Enter` to execute the cell. Take a moment to look at the data loaded into the Pandas Dataframe - it contains data about used cars such as the price (in dollars), age (in years), KM (kilometers driven) and other attributes like weather it is automatic transimission, the number of doors, and the weight."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Load the data\n\nimport numpy as np\nimport pandas as pd\nimport os\n\npathToCsvFile = os.path.join(datasetsFolderName, 'UsedCars_Clean.csv')\ndf = pd.read_csv(pathToCsvFile, delimiter=',')\nprint(df)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We are going to try and build a model that can answer the question \"Can I afford a car that is X months old and has Y kilometers on it, given I have $12,000 to spend?\". We will engineer the label for affordable. Execute the following cell."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Add the affordable feature\n\ndf['Affordable'] = np.where(df['Price']<12000, 1, 0)\ndf_affordability = df[[\"Age\",\"KM\", \"Affordable\"]]\nprint(df_affordability)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We are going to train a Logistic Regression model in Azure Databricks. This type of model requires us to standardize the scale of our training features Age and KM, so we use the `StandardScaler` from Scikit-Learn to transform these features so that they have values centered with a mean around 0 (mostly between -2.96 and 1.29). Select Step 3 and execute the code. Observe the difference in min and max values between the un-scaled and scaled Dataframes. When we use Sci-Kit Learn, these models are trained on the driver node. Execute the following cell."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Scale the numeric feature values\n\nX = df_affordability[[\"Age\", \"KM\"]].values\ny = df_affordability[[\"Affordable\"]].values\n\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X.astype(float))\n\nprint(pd.DataFrame(X).describe().round(2))\nprint(pd.DataFrame(X_scaled).describe().round(2))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Train the model by fitting a LogisticRegression against the scaled input features (X_scaled) and the labels (y). Execute the following cell."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Fit a Logistic Regression\n\nfrom sklearn import linear_model\n# Create a linear model for Logistic Regression\nclf = linear_model.LogisticRegression(C=1)\n\n# Flatten labels\ny = np.ravel(y)\n\n# we create an instance of Classifier and fit the data.\nclf.fit(X_scaled, y)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Try prediction - if you set the age to 60 months and km to 40,000, does the model predict you can afford the car? Execute the cell and find out."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Test the trained model's prediction\n\nage = 60\nkm = 40000\n\nscaled_input = scaler.transform([[age, km]])\nprediction = clf.predict(scaled_input)\n\nprint(\"Can I afford a car that is {} month(s) old with {} KM's on it?\".format(age,km))\nprint(\"Yes (1)\" if prediction[0] == 1 else \"No (0)\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now, let's get a sense for how accurate the model is. Execute the following cell. What was your model's accuracy?"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Measure the model's performance\n\nscaled_inputs = scaler.transform(X.astype(float))\npredictions = clf.predict(scaled_inputs)\nprint(predictions)\n\nfrom sklearn.metrics import accuracy_score\nscore = accuracy_score(y, predictions)\nprint(\"Model Accuracy: {}\".format(score.round(3)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "One thing that can affect the model's performance is how much data of all the labeled training data available is used to train the model. In the next cell, you define a method that uses train_test_split from Scikit-Learn that will enable you to split the data using different percentages. Execute the cell to register this function."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Define a method to experiment with different training subset sizes\n\nfrom sklearn.model_selection import train_test_split\nfull_X = df_affordability[[\"Age\", \"KM\"]]\nfull_Y = df_affordability[[\"Affordable\"]]\n\ndef train_eval_model(full_X, full_Y,training_set_percentage):\n    train_X, test_X, train_Y, test_Y = train_test_split(full_X, full_Y, train_size=training_set_percentage, random_state=42)\n    \n    # Flatten labels\n    train_Y = np.ravel(train_Y)\n    test_Y = np.ravel(test_Y)\n    \n    # Convert to float\n    train_X = train_X.astype(float)\n    test_X = test_X.astype(float)\n\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(train_X)\n    clf = linear_model.LogisticRegression(C=1)\n    clf.fit(X_scaled, train_Y)\n\n    scaled_inputs = scaler.transform(test_X)\n    predictions = clf.predict(scaled_inputs)\n    score = accuracy_score(test_Y, predictions)\n\n    return (clf, score)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Use Azure Machine Learning to log performance metrics\nIn the steps that follow, you will train multiple models using different sizes of training data and observe the impact on performance (accuracy). Each time you create a new model, you are executing a Run in the terminology of Azure Machine Learning service. In this case, you will create one Experiment and execute multiple Runs within it, each with different training percentages (and resultant varying accuracies)."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Execute the following cell to quickly verify you have the Azure Machine Learning SDK installed on your cluster. If you get a version number back without error, you are ready to proceed."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Verify AML SDK Installed\n\nimport azureml.core\nprint(\"SDK Version:\", azureml.core.VERSION)\n\n# import the Workspace class \nfrom azureml.core import Workspace",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "All Azure Machine Learning entities are organized within a Workspace. You can create an AML Workspace in the Azure Portal, but as the code in the following cell shows, you can also create a Workspace directly from code. Set the values for `subscription_id`, `resource_group`, `workspace_name` and `workspace_region` as directed by the comments. Execute Step 9. You will be prompted to log in to your Azure Subscription by the command output."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Create a workspace\n\n#Provide the Subscription ID of your existing Azure subscription\nsubscription_id = \"<your subscription>\"\n\n#Provide values for the Resource Group and Workspace that will be created\nresource_group = \"<your resource group>\"\nworkspace_name = \"<your workspace name>\"\nworkspace_region = \"<your region>\"\n\nws = Workspace.create(\n    name = workspace_name,\n    subscription_id = subscription_id,\n    resource_group = resource_group, \n    location = workspace_region,\n    exist_ok = True)\n\nprint(\"Workspace Provisioning complete.\")\nws.get_details()\nws.write_config('../')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "To begin capturing metrics, you must first create an Experiment and then call `start_logging()` on that Experiment. The return value of this call is a Run. This root run can have other child runs. When you are finished with an experiment run, use `complete()` to close out the root run. Execute the following cell to train four different models using differing amounts of training data and log the results to Azure Machine Learning."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Create an experiment and log metrics for multiple training runs\n\nfrom azureml.core.run import Run\nfrom azureml.core.experiment import Experiment\n\n# start a training run by defining an experiment\nmyexperiment = Experiment(ws, \"usedcars_training_local\")\nroot_run = myexperiment.start_logging()\n\ntraining_set_percentage = 0.25\nrun = root_run.child_run(\"Training_Set_Percentage-%0.5F\" % training_set_percentage)\nmodel, score = train_eval_model(full_X, full_Y, training_set_percentage)\nprint(\"With %0.2f percent of data, model accuracy reached %0.4f.\" % (training_set_percentage, score))\nrun.log(\"Training_Set_Percentage\", training_set_percentage)\nrun.log(\"Accuracy\", score)\nrun.complete()\n\ntraining_set_percentage = 0.5\nrun = root_run.child_run(\"Training_Set_Percentage-%0.5F\" % training_set_percentage)\nmodel, score = train_eval_model(full_X, full_Y, training_set_percentage)\nprint(\"With %0.2f percent of data, model accuracy reached %0.4f.\" % (training_set_percentage, score))\nrun.log(\"Training_Set_Percentage\", training_set_percentage)\nrun.log(\"Accuracy\", score)\nrun.complete()\n\ntraining_set_percentage = 0.75\nrun = root_run.child_run(\"Training_Set_Percentage-%0.5F\" % training_set_percentage)\nmodel, score = train_eval_model(full_X, full_Y, training_set_percentage)\nprint(\"With %0.2f percent of data, model accuracy reached %0.4f.\" % (training_set_percentage, score))\nrun.log(\"Training_Set_Percentage\", training_set_percentage)\nrun.log(\"Accuracy\", score)\nrun.complete()\n\ntraining_set_percentage = 0.9\nrun = root_run.child_run(\"Training_Set_Percentage-%0.5F\" % training_set_percentage)\nmodel, score = train_eval_model(full_X, full_Y, training_set_percentage)\nprint(\"With %0.2f percent of data, model accuracy reached %0.4f.\" % (training_set_percentage, score))\nrun.log(\"Training_Set_Percentage\", training_set_percentage)\nrun.log(\"Accuracy\", score)\nrun.complete()\n\n# Close out the experiment\nroot_run.complete()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now that you have captured history for various runs, you can review the runs. You could use the Azure Portal for this - go to the Azure Portal, find your Azure Machine Learning Workspace, select Experiments and select the UsedCars_Experiment. However, in this case we will use the AML SDK to query for the runs. Execute the following cell to view the runs and their status."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Review captured runs\n# Go to the Azure Portal, find your Azure Machine Learning Workspace, select Experiments and select the UsedCars_Experiment\n\n# You can also query the run history using the SDK.\n# The following command lists all of the runs for the experiment\nruns = [r for r in root_run.get_children()]\nprint(runs)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Train remotely using Azure ML Compute\n\nUp until now, all of your training was executed locally on the same machine running Jupyter. Now you will execute the same logic targeting a remote Azure ML Compute, which you will provision from code."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Connect to workspace\n\nws = Workspace.from_config()\nprint(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep='\\n')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Create an Azure ML Compute cluster\n\n# Create Azure ML cluster\nfrom azureml.core.compute import AmlCompute\nfrom azureml.core.compute import ComputeTarget\nimport os\n\n# choose a name for your cluster\ncluster_name = \"cpu-bai-cluster\"\ncluster_min_nodes = 1\ncluster_max_nodes = 3\nvm_size = \"STANDARD_DS11_V2\"\n\nif cluster_name in ws.compute_targets:\n    compute_target = ws.compute_targets[cluster_name]\n    if compute_target and type(compute_target) is AmlCompute:\n        print('Found existing compute target, using this compute target instead of creating:  ' + cluster_name)\n    else:\n        print(\"Error: A compute target with name \",cluster_name,\" was found, but it is not of type AmlCompute.\")\nelse:\n    print('Creating a new compute target...')\n    provisioning_config = AmlCompute.provisioning_configuration(vm_size = vm_size, \n                                                                min_nodes = cluster_min_nodes, \n                                                                max_nodes = cluster_max_nodes)\n\n    # create the cluster\n    compute_target = ComputeTarget.create(ws, cluster_name, provisioning_config)\n    \n    # can poll for a minimum number of nodes and for a specific timeout. \n    # if no min node count is provided it will use the scale settings for the cluster\n    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n    \n     # For a more detailed view of current BatchAI cluster status, use the 'status' property    \n    print(compute_target.status.serialize())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "With your cluster ready, you need to upload the training data to the default DataStore for your AML Workspace (which uses Azure Storage). "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Upload the dataset to the DataStore\n\nds = ws.get_default_datastore()\nprint(ds.datastore_type, ds.account_name, ds.container_name)\nds.upload(src_dir='../datasets', target_path='used_cars', overwrite=True, show_progress=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Next, you will need to create a training script that is similar to the code you have executed locally to train the model. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import os\nscript_folder = './script'\nos.makedirs(script_folder, exist_ok=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%%writefile $script_folder/train.py\n\nimport argparse\nimport os\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn import linear_model \nfrom sklearn.externals import joblib\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\n\nfrom azureml.core import Run\n\n# let user feed in 2 parameters, the location of the data files (from datastore), and the training set percentage to use\nparser = argparse.ArgumentParser()\nparser.add_argument('--data-folder', type=str, dest='data_folder', help='data folder mounting point')\nparser.add_argument('--training-set-percentage', type=float, dest='training_set_percentage', default=0.25, help='percentage of dataset to use for training')\nargs = parser.parse_args()\n\ndata_folder = os.path.join(args.data_folder, 'used_cars')\nprint('Data folder:', data_folder)\ndata_csv_path = os.path.join(data_folder, 'UsedCars_Clean.csv')\nprint('Path to CSV file dataset:' + data_csv_path)\n\n# Load the data\n#df = pd.read_csv('UsedCars_Clean.csv', delimiter=',')\ndf = pd.read_csv(data_csv_path)\ndf['Affordable'] = np.where(df['Price']<12000, 1, 0)\ndf_affordability = df[[\"Age\",\"KM\", \"Affordable\"]]\n\n\n# Now experiment with different training subsets\nfrom sklearn.model_selection import train_test_split\nfull_X = df_affordability[[\"Age\", \"KM\"]]\nfull_Y = df_affordability[[\"Affordable\"]]\n\ndef train_eval_model(full_X, full_Y,training_set_percentage):\n    train_X, test_X, train_Y, test_Y = train_test_split(full_X, full_Y, train_size=training_set_percentage, random_state=42)\n    \n    # Flatten labels\n    train_Y = np.ravel(train_Y)\n    test_Y = np.ravel(test_Y)\n    \n    # Convert to float\n    train_X = train_X.astype(float)\n    test_X = test_X.astype(float)\n\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(train_X)\n    clf = linear_model.LogisticRegression(C=1)\n    clf.fit(X_scaled, train_Y)\n\n    scaled_inputs = scaler.transform(test_X)\n    predictions = clf.predict(scaled_inputs)\n    score = accuracy_score(test_Y, predictions)\n\n    return (clf, score)\n\n# Acquire the current run\nrun = Run.get_context()\n\n\ntraining_set_percentage = args.training_set_percentage\nmodel, score = train_eval_model(full_X, full_Y, training_set_percentage)\nprint(\"With %0.2f percent of data, model accuracy reached %0.4f.\" % (training_set_percentage, score))\nrun.log(\"Training_Set_Percentage\", training_set_percentage)\nrun.log(\"Accuracy\", score)\n\n\n# note file saved in the outputs folder is automatically uploaded into experiment record\njoblib.dump(value=model, filename='outputs/model.pkl')\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Create an estimator that descrives the configuration of the job that will execute your model training script."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Create estimator\n#############################\nfrom azureml.train.estimator import Estimator\n\nscript_params = {\n    '--data-folder': ds.as_mount(),\n    '--training-set-percentage': 0.3\n}\n\nest_config = Estimator(source_directory=script_folder,\n                       script_params=script_params,\n                       compute_target=compute_target,\n                       entry_script='train.py',\n                       conda_packages=['scikit-learn', 'pandas'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Submit the job using the submit() method of the Experiment object. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#  Execute the estimator job\n#####################################\n\n# Create new experiment\nfrom azureml.core import Experiment\nexperiment_name = \"usedcars_training_amlcompute\"\nexp = Experiment(workspace=ws, name=experiment_name)\n\nrun = exp.submit(config=est_config)\nrun\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You can monitor the job through Azure Portal or using AML Jupyter Widget."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.widgets import RunDetails\nRunDetails(run).show()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Poll for job status\nrun.wait_for_completion(show_output=True)  # value of True will display a verbose, streaming log\n\n# Examine the recorded metrics from the run\nprint(run.get_metrics())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    },
    "name": "01 model training",
    "notebookId": 863281121960369
  },
  "nbformat": 4,
  "nbformat_minor": 1
}