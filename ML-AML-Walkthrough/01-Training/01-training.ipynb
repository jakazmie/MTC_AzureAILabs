{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1 - Training a Machine Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this lab is to demonstarte how to orchestrate machine learning **training** with Azure Machine Learning service. \n",
    "\n",
    "To keep the focus of the lab on **workflow orchestration** rather then on idiosyncrasies of a domain problem, we have chosen a relatively simple machine learning scenario.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario\n",
    "\n",
    "You will train a binary classification model to predict propensity to purchase. The dataset comes from the UCI Machine Learning repository, and it is related to direct marketing campaigns (phone calls) of a Portuguese banking institution. The goal of the model is to help with campaign planning by predicting which clients will respond positively to marketing phone calls. \n",
    "\n",
    "\n",
    "## Connect AML workspace\n",
    "\n",
    "Check the version of AML SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK Version: 1.0.2\n"
     ]
    }
   ],
   "source": [
    "# Verify AML SDK Installed\n",
    "\n",
    "import azureml.core\n",
    "print(\"SDK Version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the config file in: /data/home/demouser/notebooks/AMLsWorkshop/aml_config/config.json\n",
      "jkamlworkshop\n",
      "jkamlworkshop\n",
      "southcentralus\n",
      "952a710c-8d9c-40c1-9fec-f752138cc0b3\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "# Connect to workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare training data\n",
    "### Download the dataset\n",
    "The dataset can be downloaded from a public Azure Blob Storage container.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wget -O ../datasets/banking.csv https://azureailabs.blob.core.windows.net/banking/banking.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['banking.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "folder = '../datasets'\n",
    "filename = 'banking.csv'\n",
    "os.makedirs(folder, exist_ok=True)\n",
    "downloadCommand = 'wget -O ''{0}/{1}'' ''https://azureailabs.blob.core.windows.net/banking/{1}'''.format(folder, filename)\n",
    "print(downloadCommand)\n",
    "os.system(downloadCommand)\n",
    "os.listdir(folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 41188 entries, 0 to 41187\n",
      "Data columns (total 21 columns):\n",
      "age               41188 non-null int64\n",
      "job               41188 non-null object\n",
      "marital           41188 non-null object\n",
      "education         41188 non-null object\n",
      "default           41188 non-null object\n",
      "housing           41188 non-null object\n",
      "loan              41188 non-null object\n",
      "contact           41188 non-null object\n",
      "month             41188 non-null object\n",
      "day_of_week       41188 non-null object\n",
      "duration          41188 non-null int64\n",
      "campaign          41188 non-null int64\n",
      "pdays             41188 non-null int64\n",
      "previous          41188 non-null int64\n",
      "poutcome          41188 non-null object\n",
      "emp_var_rate      41188 non-null float64\n",
      "cons_price_idx    41188 non-null float64\n",
      "cons_conf_idx     41188 non-null float64\n",
      "euribor3m         41188 non-null float64\n",
      "nr_employed       41188 non-null float64\n",
      "y                 41188 non-null int64\n",
      "dtypes: float64(5), int64(6), object(10)\n",
      "memory usage: 6.6+ MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>...</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>emp_var_rate</th>\n",
       "      <th>cons_price_idx</th>\n",
       "      <th>cons_conf_idx</th>\n",
       "      <th>euribor3m</th>\n",
       "      <th>nr_employed</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44</td>\n",
       "      <td>blue-collar</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.4y</td>\n",
       "      <td>unknown</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>aug</td>\n",
       "      <td>thu</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.4</td>\n",
       "      <td>93.444</td>\n",
       "      <td>-36.1</td>\n",
       "      <td>4.963</td>\n",
       "      <td>5228.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53</td>\n",
       "      <td>technician</td>\n",
       "      <td>married</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>nov</td>\n",
       "      <td>fri</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>93.200</td>\n",
       "      <td>-42.0</td>\n",
       "      <td>4.021</td>\n",
       "      <td>5195.8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>management</td>\n",
       "      <td>single</td>\n",
       "      <td>university.degree</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>jun</td>\n",
       "      <td>thu</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>success</td>\n",
       "      <td>-1.7</td>\n",
       "      <td>94.055</td>\n",
       "      <td>-39.8</td>\n",
       "      <td>0.729</td>\n",
       "      <td>4991.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>apr</td>\n",
       "      <td>fri</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>-1.8</td>\n",
       "      <td>93.075</td>\n",
       "      <td>-47.1</td>\n",
       "      <td>1.405</td>\n",
       "      <td>5099.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>55</td>\n",
       "      <td>retired</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.4y</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>aug</td>\n",
       "      <td>fri</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>success</td>\n",
       "      <td>-2.9</td>\n",
       "      <td>92.201</td>\n",
       "      <td>-31.4</td>\n",
       "      <td>0.869</td>\n",
       "      <td>5076.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age          job  marital          education  default housing loan  \\\n",
       "0   44  blue-collar  married           basic.4y  unknown     yes   no   \n",
       "1   53   technician  married            unknown       no      no   no   \n",
       "2   28   management   single  university.degree       no     yes   no   \n",
       "3   39     services  married        high.school       no      no   no   \n",
       "4   55      retired  married           basic.4y       no     yes   no   \n",
       "\n",
       "    contact month day_of_week ...  campaign  pdays  previous     poutcome  \\\n",
       "0  cellular   aug         thu ...         1    999         0  nonexistent   \n",
       "1  cellular   nov         fri ...         1    999         0  nonexistent   \n",
       "2  cellular   jun         thu ...         3      6         2      success   \n",
       "3  cellular   apr         fri ...         2    999         0  nonexistent   \n",
       "4  cellular   aug         fri ...         1      3         1      success   \n",
       "\n",
       "  emp_var_rate  cons_price_idx  cons_conf_idx  euribor3m  nr_employed  y  \n",
       "0          1.4          93.444          -36.1      4.963       5228.1  0  \n",
       "1         -0.1          93.200          -42.0      4.021       5195.8  0  \n",
       "2         -1.7          94.055          -39.8      0.729       4991.6  1  \n",
       "3         -1.8          93.075          -47.1      1.405       5099.1  0  \n",
       "4         -2.9          92.201          -31.4      0.869       5076.2  1  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "folder = '../datasets'\n",
    "filename = 'banking.csv'\n",
    "pathname = os.path.join(folder, filename)\n",
    "df = pd.read_csv(pathname, delimiter=',')\n",
    "print(df.info())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset combines the information about the bank's customers with the results of previous campaigns and key economic indicators. It includes 41,188 records and 21 fields. The columns are a mix of numeric and categorical data types.\n",
    "\n",
    "The `y` column indicates whether the customer subscribed to a term deposit. This is our `target` variable or `label`. The goal of the model is to predict this column on new examples.\n",
    "\n",
    "Some information that exists in the historical dataset will not be available when planning a new campaign. \n",
    "\n",
    "Please refer to [UCI ML Repository](https://archive.ics.uci.edu/ml/datasets/bank+marketing) for more information. \n",
    "\n",
    "We will use the following features for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [\n",
    "                   # Demographic\n",
    "                   'age', \n",
    "                   'job', \n",
    "                   'education', \n",
    "                   'marital', \n",
    "                   'default', \n",
    "                   'housing', \n",
    "                   'loan', \n",
    "                   # Previous campaigns\n",
    "                   'month',\n",
    "                   'campaign',\n",
    "                   'poutcome',\n",
    "                   # Economic indicators\n",
    "                   'emp_var_rate',\n",
    "                   'cons_price_idx',\n",
    "                   'cons_conf_idx',\n",
    "                   'euribor3m',\n",
    "                   'nr_employed']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before further analysis, let's reserve a portion of data for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_raw, test_raw = train_test_split(\n",
    "   df[feature_columns + ['y']], \n",
    "   test_size = 0.2,\n",
    "   stratify = df['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look at the class distribution in training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f323f74a400>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.countplot(train_raw.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training dataset is very imbalanced. We will need to address this when configuring training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "In this section of the lab you will train a binary classification model.\n",
    "\n",
    "You will first run training on a local compute and use Azure Machine Learning Experiment to track training progress. In the following step, you will use Azure Machine Learning Compute to run  training job on more powerful cloud compute resources.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare training pipeline\n",
    "\n",
    "#### Configure feature transformations\n",
    "\n",
    "All `scikit-learn` estimators expect continuous input. Some of the features in the dataset are categorical and encoded as `strings`. We will use *one-hot* encoding to convert categorical features to numeric values. Since we are going to use *LogisticRegression*, we also need to scale numeric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# One hot encode all categorical features \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ohenc = OneHotEncoder(sparse=False)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "categorical_columns = train_raw.select_dtypes(include=[object]).columns\n",
    "numeric_columns = train_raw.drop(columns=categorical_columns.tolist()+['y']).columns\n",
    "\n",
    "ct = ColumnTransformer(\n",
    "    [('onehot', ohenc, categorical_columns),\n",
    "     ('scaler', scaler, numeric_columns)],\n",
    "     remainder='passthrough'\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure training pipeline\n",
    "\n",
    "*LogisticRegression* exposes a number of tuneable hyperparameters. Argueable, the most important setting is the inverse of regularization strength **C** . For the sake of simplicity, we will focus on tuning this hyperparameter when training our model. We will use *GridSearchCV* to automate the hyperparameter tuning process.\n",
    "\n",
    "Since the dataset is imbalanced we will set *class_weight* parameter to `balanced` to automatically adjust weights inversely proportional to class frequencies in in the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/base.py:465: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('columntransformer', ColumnTransformer(n_jobs=None, remainder='passthrough', sparse_threshold=0.3,\n",
       "         transformer_weights=None,\n",
       "         transformers=[('onehot', OneHotEncoder(categorical_features=None, categories=None,\n",
       "       dtype=<class 'numpy.float64'>, handle_unknown='error',\n",
       "    ...enalty='l2', random_state=None,\n",
       "          solver='lbfgs', tol=0.0001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Create logistic regression\n",
    "lr = LogisticRegression(solver='lbfgs', max_iter=300, class_weight='balanced')\n",
    "\n",
    "pipeline = make_pipeline(ct, lr)\n",
    "\n",
    "pipeline.fit(train_raw[feature_columns], train_raw.y)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/pipeline.py:605: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  res = transformer.transform(X)\n"
     ]
    }
   ],
   "source": [
    "y_pred = pipeline.predict(test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/pipeline.py:605: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  res = transformer.transform(X)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8259286234522942"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.score(test_raw, test_raw.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/aml/lib/python3.6/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/anaconda3/envs/aml/lib/python3.6/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=300,\n",
       "          multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n",
       "          solver='lbfgs', tol=0.0001, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'C': [0.5, 1, 2]}, pre_dispatch='2*n_jobs', refit=True,\n",
       "       return_train_score=False, scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create logistic regression\n",
    "lr = LogisticRegression(solver='lbfgs', max_iter=300, class_weight='balanced')\n",
    "\n",
    "# Set up parameter grid for C\n",
    "param_grid = {'C': [0.5, 1, 2]}\n",
    "\n",
    "# Configure grid search\n",
    "clf = GridSearchCV(lr, param_grid, \n",
    "                   cv=5, \n",
    "                   scoring='accuracy',\n",
    "                   return_train_score = False\n",
    "                   )\n",
    "\n",
    "# Start training\n",
    "X = ct.fit_transform(train_raw.drop(columns='y'))\n",
    "y = train_raw.y\n",
    "\n",
    "clf.fit(X, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C: {'C': 0.5}, score: 0.8300151745068285\n",
      "C: {'C': 1}, score: 0.8301365705614567\n",
      "C: {'C': 2}, score: 0.8301365705614567\n"
     ]
    }
   ],
   "source": [
    "for C, score in zip(clf.cv_results_['params'], clf.cv_results_['mean_test_score']):\n",
    "    print(\"C: {}, score: {}\".format(C, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run training on a local compute\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/aml/lib/python3.6/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/anaconda3/envs/aml/lib/python3.6/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.62987887, 0.63526245, 0.62668464, 0.65768194, 0.61051213])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configure RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "lr = LogisticRegression(solver='lbfgs', max_iter=300, class_weight='balanced')\n",
    "\n",
    "X = ct.fit_transform(train_raw.drop(columns='y'))\n",
    "y = train_raw.y\n",
    "\n",
    "scores = cross_val_score(\n",
    "    lr, X, y, cv=5, scoring=\"recall\")\n",
    "scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Azure Machine Learning to log performance metrics\n",
    "In the steps that follow, you will train multiple models using different values of C and observe the impact on performance (accuracy). Each time you create a new model, you are executing a Run in the terminology of Azure Machine Learning service. In this case, you will create one Experiment and execute multiple Runs within it, each with different value of C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the following cell to quickly verify you have the Azure Machine Learning SDK installed on your cluster. If you get a version number back without error, you are ready to proceed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin capturing metrics, you must first create an Experiment and then call `start_logging()` on that Experiment. The return value of this call is a Run. This root run can have other child runs. When you are finished with an experiment run, use `complete()` to close out the root run. Execute the following cell to train four different models using differing amounts of training data and log the results to Azure Machine Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an experiment and log metrics for multiple training runs\n",
    "\n",
    "from azureml.core.run import Run\n",
    "from azureml.core.experiment import Experiment\n",
    "\n",
    "# start a training run by defining an experiment\n",
    "myexperiment = Experiment(ws, \"usedcars_training_local\")\n",
    "root_run = myexperiment.start_logging()\n",
    "\n",
    "training_set_percentage = 0.25\n",
    "C = 2\n",
    "run = root_run.child_run(\"Training_Set_Percentage-%0.5F\" % training_set_percentage)\n",
    "model, score = train_eval_model(full_X, full_Y, training_set_percentage, C)\n",
    "print(\"With C={}, model accuracy reached {}\".format(C, score))\n",
    "run.log(\"Training_Set_Percentage\", training_set_percentage)\n",
    "run.log(\"C\", C)\n",
    "run.log(\"Accuracy\", score)\n",
    "run.complete()\n",
    "\n",
    "C = 1\n",
    "run = root_run.child_run(\"Training_Set_Percentage-%0.5F\" % training_set_percentage)\n",
    "model, score = train_eval_model(full_X, full_Y, training_set_percentage, C)\n",
    "print(\"With C={}, model accuracy reached {}\".format(C, score))\n",
    "run.log(\"Training_Set_Percentage\", training_set_percentage)\n",
    "run.log(\"C\", C)\n",
    "run.log(\"Accuracy\", score)\n",
    "run.complete()\n",
    "\n",
    "C = 0.75\n",
    "run = root_run.child_run(\"Training_Set_Percentage-%0.5F\" % training_set_percentage)\n",
    "model, score = train_eval_model(full_X, full_Y, training_set_percentage, C)\n",
    "print(\"With C={}, model accuracy reached {})\".format(C, score))\n",
    "run.log(\"Training_Set_Percentage\", training_set_percentage)\n",
    "run.log(\"C\", C)\n",
    "run.log(\"Accuracy\", score)\n",
    "run.complete()\n",
    "\n",
    "C = 0.5\n",
    "run = root_run.child_run(\"Training_Set_Percentage-%0.5F\" % training_set_percentage)\n",
    "model, score = train_eval_model(full_X, full_Y, training_set_percentage, C)\n",
    "print(\"With C={}, model accuracy reached {})\".format(C, score))\n",
    "run.log(\"Training_Set_Percentage\", training_set_percentage)\n",
    "run.log(\"C\", C)\n",
    "run.log(\"Accuracy\", score)\n",
    "run.complete()\n",
    "\n",
    "# Close out the experiment\n",
    "root_run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have captured history for various runs, you can review the runs. You could use the Azure Portal for this - go to the Azure Portal, find your Azure Machine Learning Workspace, select Experiments and select the UsedCars_Experiment. However, in this case we will use the AML SDK to query for the runs. Execute the following cell to view the runs and their status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review captured runs\n",
    "# Go to the Azure Portal, find your Azure Machine Learning Workspace, select Experiments and select the UsedCars_Experiment\n",
    "\n",
    "# You can also query the run history using the SDK.\n",
    "# The following command lists all of the runs for the experiment\n",
    "runs = [r for r in root_run.get_children()]\n",
    "print(runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train remotely using Azure ML Compute\n",
    "\n",
    "Up until now, all of your training was executed locally on the same machine running Jupyter. Now you will execute the same logic targeting a remote Azure ML Compute, which you will provision from code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Azure ML Compute cluster\n",
    "\n",
    "# Create Azure ML cluster\n",
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "import os\n",
    "\n",
    "# choose a name for your cluster\n",
    "cluster_name = \"cpu-bai-cluster\"\n",
    "cluster_min_nodes = 1\n",
    "cluster_max_nodes = 3\n",
    "vm_size = \"STANDARD_DS11_V2\"\n",
    "\n",
    "if cluster_name in ws.compute_targets:\n",
    "    compute_target = ws.compute_targets[cluster_name]\n",
    "    if compute_target and type(compute_target) is AmlCompute:\n",
    "        print('Found existing compute target, using this compute target instead of creating:  ' + cluster_name)\n",
    "    else:\n",
    "        print(\"Error: A compute target with name \",cluster_name,\" was found, but it is not of type AmlCompute.\")\n",
    "else:\n",
    "    print('Creating a new compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = vm_size, \n",
    "                                                                min_nodes = cluster_min_nodes, \n",
    "                                                                max_nodes = cluster_max_nodes)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, provisioning_config)\n",
    "    \n",
    "    # can poll for a minimum number of nodes and for a specific timeout. \n",
    "    # if no min node count is provided it will use the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "    \n",
    "     # For a more detailed view of current BatchAI cluster status, use the 'status' property    \n",
    "    print(compute_target.status.serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With your cluster ready, you need to upload the training data to the default DataStore for your AML Workspace (which uses Azure Storage). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the dataset to the DataStore\n",
    "\n",
    "ds = ws.get_default_datastore()\n",
    "print(ds.datastore_type, ds.account_name, ds.container_name)\n",
    "ds.upload(src_dir='../datasets', target_path='used_cars', overwrite=True, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you will need to create a training script that is similar to the code you have executed locally to train the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "script_folder = './script'\n",
    "os.makedirs(script_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $script_folder/train.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import linear_model \n",
    "from sklearn.externals import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from azureml.core import Run\n",
    "\n",
    "# let user feed in 2 parameters, the location of the data files (from datastore), and the training set percentage to use\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data-folder', type=str, dest='data_folder', help='data folder mounting point')\n",
    "parser.add_argument('--training-set-percentage', type=float, dest='training_set_percentage', default=0.25, help='percentage of dataset to use for training')\n",
    "parser.add_argument('--C', type=float, dest='C', default=1, help='regularization')\n",
    "args = parser.parse_args()\n",
    "\n",
    "data_folder = os.path.join(args.data_folder, 'used_cars')\n",
    "print('Data folder:', data_folder)\n",
    "data_csv_path = os.path.join(data_folder, 'UsedCars_Clean.csv')\n",
    "print('Path to CSV file dataset:' + data_csv_path)\n",
    "\n",
    "# Load the data\n",
    "#df = pd.read_csv('UsedCars_Clean.csv', delimiter=',')\n",
    "df = pd.read_csv(data_csv_path)\n",
    "df['Affordable'] = np.where(df['Price']<12000, 1, 0)\n",
    "df_affordability = df[[\"Age\",\"KM\", \"Affordable\"]]\n",
    "\n",
    "\n",
    "# Now experiment with different training subsets\n",
    "from sklearn.model_selection import train_test_split\n",
    "full_X = df_affordability[[\"Age\", \"KM\"]]\n",
    "full_Y = df_affordability[[\"Affordable\"]]\n",
    "\n",
    "def train_eval_model(full_X, full_Y,training_set_percentage, C):\n",
    "    train_X, test_X, train_Y, test_Y = train_test_split(full_X, full_Y, train_size=training_set_percentage, random_state=42)\n",
    "    \n",
    "    # Flatten labels\n",
    "    train_Y = np.ravel(train_Y)\n",
    "    test_Y = np.ravel(test_Y)\n",
    "    \n",
    "    # Convert to float\n",
    "    train_X = train_X.astype(float)\n",
    "    test_X = test_X.astype(float)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(train_X)\n",
    "    clf = linear_model.LogisticRegression(C=C, solver='lbfgs')\n",
    "    clf.fit(X_scaled, train_Y)\n",
    "\n",
    "    scaled_inputs = scaler.transform(test_X)\n",
    "    predictions = clf.predict(scaled_inputs)\n",
    "    score = accuracy_score(test_Y, predictions)\n",
    "\n",
    "    return (clf, score)\n",
    "\n",
    "# Acquire the current run\n",
    "run = Run.get_context()\n",
    "\n",
    "\n",
    "training_set_percentage = args.training_set_percentage\n",
    "C = args.C\n",
    "model, score = train_eval_model(full_X, full_Y, training_set_percentage, C)\n",
    "print(\"With C={}, model accuracy reached {})\".format(C, score))\n",
    "run.log(\"Training_Set_Percentage\", training_set_percentage)\n",
    "run.log(\"C\", C)\n",
    "run.log(\"Accuracy\", score)\n",
    "\n",
    "\n",
    "# note file saved in the outputs folder is automatically uploaded into experiment record\n",
    "joblib.dump(value=model, filename='outputs/model.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an estimator that descrives the configuration of the job that will execute your model training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create estimator\n",
    "#############################\n",
    "from azureml.train.estimator import Estimator\n",
    "\n",
    "script_params = {\n",
    "    '--data-folder': ds.as_mount(),\n",
    "    '--training-set-percentage': 0.3,\n",
    "    '--C': 2\n",
    "}\n",
    "\n",
    "est_config = Estimator(source_directory=script_folder,\n",
    "                       script_params=script_params,\n",
    "                       compute_target=compute_target,\n",
    "                       entry_script='train.py',\n",
    "                       conda_packages=['scikit-learn', 'pandas'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit the job using the submit() method of the Experiment object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Execute the estimator job\n",
    "#####################################\n",
    "\n",
    "# Create new experiment\n",
    "from azureml.core import Experiment\n",
    "experiment_name = \"usedcars_training_amlcompute\"\n",
    "exp = Experiment(workspace=ws, name=experiment_name)\n",
    "\n",
    "run = exp.submit(config=est_config)\n",
    "run\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can monitor the job through Azure Portal or using AML Jupyter Widget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Poll for job status\n",
    "run.wait_for_completion(show_output=True)  # value of True will display a verbose, streaming log\n",
    "\n",
    "# Examine the recorded metrics from the run\n",
    "print(run.get_metrics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "name": "01 model training",
  "notebookId": 863281121960369
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
